__***Avec le monde d'avant l'IA on apprenait Ã  faire puis on faisait. Avec le monde d'aujourd'hui on fait faire Ã  l'IA, puis on apprend Ã  partir de ce que l'IA a fait, on finit donc par savoir faire aussi. Du coup plus besoin de prof, il ne suffit que de vouloir faire pour parvenir Ã  nos fins !! The sky is the limit !!!***__

**TO DO  â¬œ / DONE âœ…** / **en cours** âš™ï¸ / **tools** ğŸ› ï¸ / **innovations** ğŸ’¡
|        |        |  |
|-------------------|-------------------|-------------------|
| - Prompting (12) âš™ï¸| - Site web sur Github (7) âœ…      | Traitement d'images(13)  âš™ï¸ |
| - ğŸ› ï¸ Docker  âœ…| - Simple API locale (1) âœ…      | Speech to text  â¬œ (30)|
| - ğŸ› ï¸[Github](https://github.com/) (10) âœ…| -  (5)Programmation no code (AIDER âœ…), (MicroAgent â¬œ)     |- Web scrapping via python (11) âš™ï¸      |
| - ğŸ› ï¸ GPT-4o âœ… et [playground](https://platform.openai.com/playground/chat?models=gpt-4o)| -  ğŸ› ï¸ LM Studio âœ…       |- RAG  (16)  âš™ï¸     |
| - ğŸ› ï¸ Anthropic [chat](https://claude.ai/new) & [API](https://www.anthropic.com/api-bk) (Sonnet 3.5) âœ…| - Text to speech via python: payant(3. âœ…) et gratuit (19. â¬œ)     |-  GPTs  â¬œ  |
| - ğŸ› ï¸[Perplexity](https://www.perplexity.ai/)   âœ…|- Text to vidÃ©o et traitements (3)    âš™ï¸       |-  Agents (21) â¬œ  |
| - ğŸ› ï¸ Comfyui   âœ…| - â¬œ:  Tutoriel video automatique     |- Function calling (Gorilla) â¬œ   |
| - ğŸ› ï¸ Copilot âœ…| -  CrÃ©ation d'images consistantes  âœ…    |- Text to CAD (9) âš™ï¸      |
| - ğŸ› ï¸ Anaconda âœ…| - CrÃ©ation de tutoriel vidÃ©o  âœ…       |- Serveur local â¬œ    |
| - ğŸ› ï¸[Hedra](https://www.hedra.com/) âœ… | - VidÃ©o-livre narratif gÃ©nÃ©rÃ© Ã  partir de texte (8) âœ…      |- Remote PC (18) âœ…   |
| - ğŸ› ï¸Mistral| - LLM via python (2)  âœ…      |-  [LLM en //](https://youtu.be/6hG39mr9c0k?si=jwaLEMEDurlWbHuF)   â¬œ  |
| - ğŸ› ï¸[Deepseek api](https://www.deepseek.com/) â¬œ  | - Text to image local (6)   âœ…     |-  Open interpreter â¬œ  |
|- ğŸ› ï¸ [Groq](https://groq.com/) âœ… |ğŸ› ï¸ Hugging Face âœ…  |- Coder Ã  partir de GitHub, HF, papers with code  â¬œ    |
|- ğŸ› ï¸VS studio âœ… |- ğŸ› ï¸ Kaggle âœ…   |- ğŸ› ï¸ Colab  âœ…  |
|- CaractÃ¨res consistants (14) âš™ï¸ |-  Apprendre de faÃ§on active (15) âš™ï¸  |- âš™ï¸ DÃ©tection automatique avec Sonnet (17)  |
|- ğŸ› ï¸ [Poe](https://poe.com/) âœ… |-  ğŸ› ï¸ Web Apps by [123apps](https://123apps.com/) :Edit, Convert, Create |- ğŸ› ï¸ [VAPI](https://docs.vapi.ai/introduction) : your remote phone assistant |
|- â¬œ : ğŸ› ï¸ Ã  Ã©valuer (20)  |-â¬œ : Fabrication low cost (26)  |- â¬œ : Avatar parlant ou chantant gratuit (28)  |
|- âš™ï¸: LearnAnything (22) ğŸ’¡ | - âš™ï¸: Text to Graph (23) ğŸ’¡   |- â¬œ : RAG to Graph     |
|- âš™ï¸ : API de calcul scientifique (24) ğŸ’¡ |- â¬œ : DÃ©ploiement du chatbot (25)      | Bot assistant multimodal (27)   |
|- â¬œ : [OpenedAI Vision (~ GPT4-vision)](https://www.youtube.com/watch?v=61F_4zfs_Jg)|- â¬œ : Traitement images et video (31)   |-  |
| - â¬œ : Automatisation de tÃ¢ches (32) | AmÃ©lioration des performances d'un LLM (33)  |  |



# AI-automation
Tout faire avec l'IA. Elle fait le boulot sous votre contrÃ´le et vous forme Ã  comprendre comment tout Ã§a fonctionne.

L'idÃ©e est de construire pas Ã  pas une "baquette magique" apte Ã  tout faire. Nous nous bornerons Ã  vous indiquer:
- Les bons outils Ã  utiliser,
- Comment bien poser votre problÃ¨me
- Comment apprendre cette nouvelle faÃ§on de travailler , en comprenant comment la magie opÃ©re.
- Cela passe par l'emploi d'outils pour :
  - Ne pas rÃ©inventer la roue, donc voir si notre problÃ¨me n'est pas dÃ©jÃ  rÃ©solu. c'est le "**web scraping**" en particulier sur Github, Hugging Face ou Papers with Code
  - RÃ©cupÃ©rer l'Ã©tat de l'art c'est le "web scraping" de site cpomme Arxiv en particulier et le **RAG** pour stocker le savoir
  - Le programmer en "no code" avec les **coding assistants**
  - Utiliser des **agents** collaborant entre eux pour rÃ©soudre les problÃ¨mes les plus complexes 



## Les incontournables utilisables sans rien automatiser. 

- [Mieux vaut regarder les benchmarks pour choisir](https://klu.ai/glossary/mmlu-pro-eval)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/57f69c6d-9505-4b23-82a7-eee9025e392e" width="600" >

  - â¬œ :
  
    - [QWEN2](https://qwenlm.github.io/blog/qwen2) est semble-t-il trÃ¨s performant (128 k de contexte)

    -  [gemma2](https://artificialanalysis.ai/models/gemma-2-9b)
      
    -  ![image](https://github.com/user-attachments/assets/7e4a4f2d-373b-437e-82dd-dcfc22d4df21)

    -  Mise en oeuvre des modÃ¨les locaux avec [HuggingFace candle](https://www.youtube.com/watch?v=8hpYedvjrxE) : Ultra-rapide, dÃ©veloppÃ© en Rust  

- Pour les maths et le raisonnement en local : [mathstral](https://mistral.ai/news/mathstral/)
![image](https://github.com/user-attachments/assets/50f68eed-55ce-411c-8878-31c09fbcd57a)

- Codestral Mamba, de Mistral AI, est un modÃ¨le innovant qui peut traiter les sÃ©quences rapidement, quelle que soit leur longueur, capable de gÃ©rer jusqu'Ã  256k tokens avec des performances Ã©quivalentes aux modÃ¨les transformateurs de pointe, particuliÃ¨rement efficace pour les applications de productivitÃ© de code
<img src="https://github.com/user-attachments/assets/146d165a-645a-453b-b9d5-f2f671c3c2f6" width="70%">

- Donc utiliser **Sonnet 3.5** et **GPT-4o** pour avoir les meilleures rÃ©ponses Ã  nos questions.
- [**Perplexity**](https://www.perplexity.ai/) est un autre incontournable pour surfer sur le web ( que nous contournerons quand mÃªme plus tard ! ğŸ˜Š)
- [**Harpa**](Harpa.ai) permet d'interagir avec une page web ou une video YouTube
- **Copilot de Microsoft**
- Je vous laisse le soin de tester ces diffÃ©rents logiciels qui mÃªme dans leurs versions gratuites amÃ©lioreront sensiblement votre productivitÃ©.

## Les incontournables pour automatiser:
- Les mÃªmes (ou leur Ã©quivalent) accessibles Ã  l'intÃ©rieur d'un code Python. 
- Des outils produisant et mettant en oeuvre le code Ã  notre place
- Des outils pour produire automatiquement des vidÃ©os qui nous servirons Ã  apprendre ce que fait l'IA
- Des outils pour [appeler des fonctions externes](https://gorilla.cs.berkeley.edu/leaderboard.html)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/084708bc-b8f1-469e-94bd-32d48cc6cf50" width="600" />

## Architecture gÃ©nÃ©rale:
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/17a47369-f026-46b6-a11c-0dc0d48f35de" width="600" />

- Le coeur du systÃ¨me, les **LLM** (Large language model)  recoivent du texte, le traite et fournissent du texte en retour. Le texte d'entrÃ©e doit Ãªtre tel qu'il exprime clairement et concrÃ¨tement nos attentes (c'est le prompting)
- Le texte en retour peut Ãªtre formatÃ© pour correspondre Ã  une rÃ©ponse de type texte brut, JSON,  markdown, HTML , code , API ... suivant le post processing envisagÃ©.

1. ## CrÃ©ation automatique d'une API web qui peut effectuer deux opÃ©rations mathÃ©matiques :

- Additionner deux nombres
- Multiplier deux nombres

- L'application doit Ãªtre conteneurisÃ©e avec Docker pour faciliter son dÃ©ploiement et son exÃ©cution.
- Vous voulez un guide Ã©tape par Ã©tape pour crÃ©er cette application, en partant de zÃ©ro, sans aucun outil prÃ©installÃ© sur votre ordinateur.
- L'objectif final est de pouvoir non seulement crÃ©er cette application, mais aussi de pouvoir la partager facilement. 
- Vous voulez que n'importe qui puisse la tÃ©lÃ©charger et la lancer sur son propre ordinateur, quelle que soit sa configuration.
- Vous avez besoin d'instructions claires sur comment lancer l'application et comment la reproduire sur un autre PC.
- [**Dialogue avec sonnet 3.5** pour mettre en oeuvre la solution en "manuel"](https://claude.ai/chat/a71daeb6-5875-4ecb-9dc6-7dce126afde0) 
- Nous verrons plus tard comment automatiser la mise en place de ce type d'application en automatique avec AIDER
  
## Faisons le tutoriel correspondant sous forme de vidÃ©o
- L'idÃ©e est de partir de la synthÃ¨se rÃ©capitulÃ©e par sonnet 3.5 de notre programme prÃ©cÃ©dent pour en faire un tuto.
- Pour cela on Ã©tablit un [dialogue avec sonnet 3.5](https://claude.ai/chat/08fb3cc8-5cb4-45ed-9132-953e30ecf792) pour dÃ©grossir le problÃ¨me:
    - Pour crÃ©er les planches HTML support,
    - Le texte des voice over,
    - Le prompt pour produire les images
    - Le code python pour stocker ces donnÃ©es dans des rÃ©pertoires
    - Le code python crÃ©ant les MP3 et les images et qui les stockent
    - Le code python qui fait l'assemblage
- Ce dÃ©grossissage montre qu'il est prÃ©fÃ©rable d'avancer pas Ã  pas en construisant et validant pas Ã  pas le code python correspondant, ce que nous allons faire maintenant avec un nouveau thread sonnet 3.5.
2. **CrÃ©er un code python permettant de dialoguer avec sonnet 3.5**
  - On utilisera Visual Studio Code pour la mise en oeuvre et pour tester les codes
  - On utilisera anaconda pour crÃ©er un environnement logiciel spÃ©cifique. Nous utiliserons l'environnement teambot dÃ©jÃ  crÃ©er avec `conda activate teambot` dans un terminal 
  - On crÃ©e un rÃ©pertoire de travail video-maker dans lequel on met le fichier .env avec nos clefs API, ainsi que les fichiers requirements.txt et anthropic-api-hello-world.py crÃ©er par sonnet 3.5
  - Le dialogue fonctionne :
    
```bash
(base) PS C:\Users\test\Documents\AI_Automation\video_maker> conda activate teambot

(teambot) PS C:\Users\test\Documents\AI_Automation\video_maker> python anthropic-api-hello-world.py
Claude dit: [TextBlock(text='Bonjour !', type='text')]
```

3. [**CrÃ©ation d'une vidÃ©o Ã  partir d'un texte**](https://claude.ai/chat/c33dece9-e5ab-4206-98c6-de644cb1d731)  
- Ce projet automatise la crÃ©ation de vidÃ©os Ã©ducatives Ã  partir de contenu textuel, utilisant diverses technologies et APIs. Le processus se dÃ©roule en plusieurs Ã©tapes intÃ©grÃ©es dans un script Python unique :
  - Conversion du texte :
    - Lit le contenu du fichier PLACE_HOLDER_TEXTE_VIDEO.txt.
    - Utilise l'API Claude d'Anthropic pour convertir le texte en structure JSON de diapositives.
  - Traitement des diapositives :
    - GÃ©nÃ¨re un fichier HTML structurÃ© avec CSS intÃ©grÃ© pour chaque diapositive.
    - CrÃ©e un texte de voix off avec Claude.
    - Produit une image illustrative via l'API DALL-E d'OpenAI.
    - GÃ©nÃ¨re un fichier audio de la voix off avec l'API Text-to-Speech d'OpenAI.
  - CrÃ©ation des vidÃ©os :
    - Capture une image du HTML rendu avec Selenium.
    - Combine l'image et l'audio en utilisant MoviePy pour chaque diapositive.
  - AgrÃ©gation finale :
    - Assemble toutes les vidÃ©os individuelles en une seule vidÃ©o.
    - Ajoute des transitions entre les diapositives.
  - Le projet utilise Python avec diverses bibliothÃ¨ques (BeautifulSoup, Requests, Pillow, MoviePy) et APIs (Anthropic, OpenAI). Cette approche intÃ©grÃ©e offre une solution complÃ¨te et efficace pour la production automatisÃ©e de contenu vidÃ©o Ã©ducatif, de la conversion du texte Ã  la crÃ©ation de la vidÃ©o finale.
âš™ï¸ : [**Text to video de longue durÃ©e**](https://video-infinity.tanzhenxiong.com/) en open source
- [Rajout d'effet spÃ©ciaux sonores](https://github.com/open-mmlab/FoleyCrafter):FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds
-  â¬œ : [Echange de visages dan une vidÃ©o](https://www.youtube.com/watch?v=Iy0PaElchlg)
4. **Point d'Ã©tape:**
- Nous avons rÃ©ussi Ã  mettre en oeuvre une applicatoin complexe sans coder une seule ligne. Cependant ce faisant nous avons dÃ©tectÃ© des pistes pour augmenter encore notre productivitÃ©
  - **Automatiser les itÃ©rations de dÃ©buggage** , ce qui nous a fait perdre le plus de temps dans la mise au point du code
  - Passer Ã  l'open source, en particulier pour la crÃ©ation d'image qui constitue le poste de dÃ©pense le plus Ã©levÃ© pour la crÃ©ation d'une vidÃ©o
  - Faire du web scraping pour voir si notre problÃ¨me n'est pas dÃ©jÃ  rÃ©solu par ailleurs
 
5. **Coding assistant**:
- âœ…: AIDER+Sonnet**
  - On crÃ©e le rÃ©pertoire ```coding_assistant``` et on lance ```conda activate teambot```
  - On suit les [instructions d'installation](https://github.com/paul-gauthier/aider) 
  - Mais il faut l'adapter au terminal powershell :```$env:ANTHROPIC_API_KEY="sk... "```
  - AIDER rÃ©pond Ã  nos directives et adapte en consÃ©quence un repository qui a Ã©tÃ© clonÃ© localement.
  - Il conserve un logbook des actions entreprises (```.aider.chat.history.md```) et un LLM comme Sonnet 3.5 ou GPT-4o peut alors en faire la synthÃ¨se:
    ```J'ai eu des problÃ¨mes qui ont Ã©tÃ© rÃ©solu dans le document joint, fais en la synthÃ¨se```
  - AIDER est [SOTA](https://aider.chat/docs/leaderboards/) avec Sonnet 3.5 mais il est aussi trÃ¨s performant avec DeepSeekCoder
  - â¬œ :[AIDER avec deepSeek](https://youtu.be/Y-_0VkMUiPc?si=zAPZQrYj6yrMHhne)
-  â¬œ [**Micr-oagent**](https://github.com/BuilderIO/micro-agent) : itÃ©ration automatique sur des cas tests
-  â¬œ [**CodeGeeX4**](https://github.com/THUDM/CodeGeeX4) : en plugin de VS code

- 
6. [**Text to image local dans docker**](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
- Il faut "alimenter" comfyui en y rajoutant les chkpoints requis Ã  placer dans le rÃ©pertoire : ```C:\Users\test\Documents\AI_Automation\coding_assistant\comfyui\storage\ComfyUI\models\checkpoints```
- Il n'y a pas de consensus clair sur un seul Â« meilleur Â» point de contrÃ´le pour ComfyUI, car cela dÃ©pend beaucoup de vos prÃ©fÃ©rences personnelles et du type d'image que l'on souhaite gÃ©nÃ©rer. Cependant, plusieurs points de contrÃ´le sont frÃ©quemment recommandÃ©s pour leur qualitÃ© :
  - SDXL (Stable Diffusion XL) : C'est un modÃ¨le de base trÃ¨s performant, particuliÃ¨rement bon pour le rÃ©alisme et la qualitÃ© gÃ©nÃ©rale des images.
  - Juggernaut XL : Souvent citÃ© comme l'un des meilleurs pour le photorÃ©alisme
  - Dreamshaper : ApprÃ©ciÃ© pour sa polyvalence et sa qualitÃ©, particuliÃ¨rement dans sa version Turbo
  - Vision rÃ©aliste : Excellent pour gÃ©nÃ©rer des humains rÃ©alistes.
  - RealVis XL : Ã‰galement recommandÃ© pour le photorÃ©alisme
- La gÃ©nÃ©ration d'images peut se faire via une requÃªte API comme le montre  [basic_api_exemple.py](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
7. [**CrÃ©er son site en ligne avec Github**:

Pour mettre en place un site personnel avec GitHub Pages, voici les Ã©tapes que vous devez suivre :

1. [**CrÃ©ez un compte GitHub** :]([https://chatgpt.com/c/2a5fd138-49c0-42bb-a057-a831e6dbc5ea](https://chatgpt.com/c/37b0d84b-d7bd-4455-b5ae-44082f81226c))
- Il faut crÃ©er un dÃ©pot public sur Github
- CrÃ©ez un fichier `index.html` avec un contenu ```<html><head><title>Mon Site</title></head><body><h1>Bonjour Monde JPB !</h1></body></html>```
- Le cloner en local ```git clone https://github.com/jpbrasile/github.io```
- **Publiez vos modifications** :
     - Ajoutez les fichiers modifiÃ©s Ã  votre dÃ©pÃ´t :
       ```
       git add --all
       ```
     - Faites un commit des modifications :
       ```
       git commit -m "Initial commit"
       ```
     - Poussez les modifications sur GitHub :
       ```
       git push -u origin main
       ```
  - Il faut faire "sign in with a code" pour parvenir Ã  valider le push
    
  Une fois ces Ã©tapes terminÃ©es, votre site sera en ligne Ã  l'adresse [https://username.github.io/github.io/](https://jpbrasile.github.io/github.io/).

  8- **CrÃ©er un story teller automatique**
- [SEED](https://github.com/TencentARC/SEED-Story) : SEED-Story, un modÃ¨le de langage multimodal (MLLM) capable de gÃ©nÃ©rer de longues histoires multimodales composÃ©es de textes narratifs riches et cohÃ©rents, accompagnÃ©s d'images cohÃ©rentes en termes de personnages et de style, basÃ© sur SEED-X. Avec StoryStream, un vaste ensemble de donnÃ©es spÃ©cialement conÃ§u pour l'entraÃ®nement et l'Ã©valuation comparative de la gÃ©nÃ©ration d'histoires multimodales.
<img src="https://github.com/user-attachments/assets/8b2bb448-b9ff-452d-b316-885dda145ae9" width=350%">

- La premiÃ¨re Ã©tape consiste Ã  produire des images consistantes afin que les personnages ou objets reproduits sur plusieurs diapositives soient quasi-identiques:
  - Nous avons crÃ©Ã© un "mÃ©ta-prompt" qui Ã  partir d'un descriptif simple <situation> fournit des images consistantes:
  - MÃ©ta-prompt:

```
Prompt_Structure:
{scene_setting}. {entity1_name} ({entity1_traits}) {action1} {object1_name} ({object1_traits}). {entity2_name} ({entity2_traits}) {action2}. {atmosphere}

Definitions:
- scene_setting: Brief description of location and context
- entity1_name: Identifier for the first entity (e.g., man_1, woman_1, dog_1)
- entity1_traits: age, physical_appearance, clothing
- action1: Main action of entity1
- object1_name: Identifier for the main object (e.g., watch_1, map_1)
- object1_traits: material, distinctive_features, size
- entity2_name: Identifier for the second entity
- entity2_traits: age, physical_appearance, clothing/characteristics
- action2: Main action of entity2
- atmosphere: General ambiance and shared activity

Instructions:
1. Replace each element with specific details consistent with the provided <situation>.
2. Maintain coherence between entities, their actions, and the context.
3. Use unique identifiers for entities and objects to facilitate reference and reuse.

Example:
Cafe_table_1 in a bustling coffee shop. Man_1 (25 years old, short brown hair, white shirt and blue jeans) carefully examines Watch_1 (antique gold, floral engravings, normal size). Woman_1 (70 years old, gray hair in a bun, red floral dress and round glasses) leans forward with interest. They are absorbed in their animated conversation.
```
- Sonnet a rÃ©alisÃ© le prompt spÃ©cifique Ã  partir de ```<situation> A man walking his dog in the forest```
```
Forest_trail_1 in a lush, shadowy woodland with dappled sunlight. Man_1 (45 years old, salt-and-pepper hair, rugged stubble, green waterproof jacket and khaki hiking pants) studies Map_1 (weatherproof paper, colorful trail markings, slightly creased) while striding purposefully. Dog_1 (6 years old, German Shepherd, glossy black and tan coat, red nylon collar with tags) trots eagerly ahead, nose twitching at the scent of pine and earth. They are enveloped in a serene atmosphere of natural exploration, with a gentle breeze rustling through the canopy above.
``` 
  - VoilÃ  le rÃ©sultat fournit par copilot pour la crÃ©ation d'image avec copilot:
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/497bf3b4-b95d-451a-8775-1c99a2f5ac5d)
  - et avec leonardo.ai (qui oublie la carte et met le "collier rouge" sur le vieil homme) :
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/ed959f12-0774-472a-950f-e810baa0c861)

  - La deuxiÃ¨me Ã©tape consiste Ã  avoir le script de la vidÃ©o, c'est Ã  dire tous les Ã©lÃ©ments textuels qui permettront la crÃ©ation automatique de la vidÃ©o.  
    - Trouver des idÃ©es d'un livre : le prompting est essentiel pour y parvenir: [`trouve dix livres intÃ©ressants pour un enfant de 12 ans sur les super hÃ©ros`](https://www.perplexity.ai/search/trouve-dix-livres-interessants-krUHHsFMRMGL1qYUI5.1yQ)
    - Le livre Ã©tant choisi on rÃ©cupÃ¨re la trame en alimentant sonnet 3.5 de toutes les informations rÃ©cupÃ©rÃ©es sur  le net et en lui demandant [`imagine le script de ce livre, en le structurant sous une forme compacte et structurÃ©e qui sera lu uniquement par un llm pas par un humain. utilise les ressorts classiques d'Ã©criture pour ce style de livre`](https://claude.ai/chat/72358672-7eb9-416a-a16a-dcad3a129953) puis la suite de la discussion avec sonnet 3.5 nous permet de gÃ©nÃ©rer un fichier story.JSON qui contient Ã  la fois le voice over et le descriptif des images en conservant la consistance des personnages.
    - Nous allons maintenant utiliser AIDER pour transformer le programme apte Ã  gÃ©nÃ©rÃ©er des tutoriels vidÃ©o Ã  ce nouveau besoin.
      - Nous avons synchronisÃ© notre dÃ©pot video_maker Ã  github afin que AIDER puisse le prendre en compte
      - Nous avons crÃ©er une appli qui lance AIDER dans l'environnement conda teambot avec `python launch_aider.py` . Cela permet de rÃ©cupÃ©rer la clef Anthropic Ã  partir de .env et de lancer AIDER
      - AIDER a alors rÃ©pondu Ã  notre demande de modification et nous fournit ici la synthÃ¨se de ses modifications
        - 1. Dans le fichier `2JSONV2.py` :
         - J'ai ajoutÃ© un nouveau mode de test local (mode 3) dans la fonction `main()`.
         - J'ai crÃ©Ã© des fonctions de remplacement (mock) pour `generate_image` et `text_to_speech` pour le mode de test local.
         - J'ai modifiÃ© la logique de sÃ©lection du mode pour inclure le nouveau mode de test.

        - 2. Dans le fichier `requirements.txt` :
         - J'ai spÃ©cifiÃ© des versions prÃ©cises pour chaque dÃ©pendance.
         - J'ai remplacÃ© 'dotenv' par 'python-dotenv'.
         - J'ai supprimÃ© 'peewee' car il ne semblait pas Ãªtre utilisÃ© dans le script actuel.

        - 3. Je n'ai pas modifiÃ© les fichiers `PLACE_HOLDER_TEXTE_VIDEO.txt` et `story.json`.

        Ces modifications visent Ã  amÃ©liorer la testabilitÃ© du script et Ã  assurer une meilleure reproductibilitÃ© de l'environnement de dÃ©veloppement. Le mode de test local permet de tester le flux de travail sans faire d'appels API rÃ©els, ce qui peut Ãªtre utile pour le dÃ©bogage et les tests rapides.   

9. **Text to CAD**
- âš™ï¸ :Sonnet 3.5 semble Ãªtre capable de crÃ©er un [programme python capable de gÃ©nÃ©rer des formes complexes](https://claude.ai/chat/91026ba9-f74b-4622-b215-3148ada38543)
- â¬œ :  Par ailleurs [CadQuery](https://github.com/CadQuery/cadquery) semble intÃ©ressant Ã  Ã©valuer 
  - ğŸ› ï¸ CadQuery : Module Python intuitif pour crÃ©er des modÃ¨les paramÃ©triques 3D.
  - âœï¸ Scripts courts : Ã‰crire des scripts simples pour produire des modÃ¨les de haute qualitÃ©.
  - ğŸ†š Comparaison OpenSCAD :
  - ğŸ“œ Utilise Python : AccÃ¨s Ã  de nombreuses bibliothÃ¨ques et IDE.
  - ğŸ”§ Noyau OCCT : Plus puissant que CGAL, supporte NURBS, splines, import/export STEP.
  - â±ï¸ Scripts concis : Moins de code nÃ©cessaire grÃ¢ce Ã  des fonctionnalitÃ©s de positionnement avancÃ©es.
  - ğŸš€ GÃ©nÃ©ration rapide : CrÃ©e des fichiers STL, STEP, AMF et 3MF plus rapidement.
  - ğŸ’» IntÃ©gration facile : ConÃ§u comme bibliothÃ¨que Python sans GUI, idÃ©al pour serveurs et scripts scientifiques.
  - ğŸ›¡ï¸ Avantages :
    - ğŸ”„ ModÃ¨les paramÃ©triques facilement personnalisables.
    - ğŸ–¨ï¸ Sortie de formats CAD de haute qualitÃ© (STEP, DXF, etc.).
    - ğŸ§© Assemblages imbriquÃ©s Ã  partir de piÃ¨ces individuelles.
  - ğŸš€ Version 2.0 :
  - ğŸ”„ BasÃ©e sur OCCT : Plus de contrÃ´le et de flexibilitÃ©, malgrÃ© une complexitÃ© accrue.
- âš™ï¸ :[**BeeGraphy**](https://www.youtube.com/watch?v=MdNp6nQgqgU) : Text to CAD via remote API. Tutorial en cours 

- âš™ï¸ A voir le [tutorial](https://zoo.dev/docs/tutorials/text-to-cad) 

10. **Utilisation de Github**
- Voici les Ã©tapes pour transformer votre rÃ©pertoire local en repository GitHub et synchroniser les deux avec Git :
  - CrÃ©ez un nouveau repository sur GitHub :
    - Connectez-vous Ã  votre compte GitHub
    - Cliquez sur "New repository"
    - Donnez un nom Ã  votre repository
    - Ne cochez pas "Initialize this repository with a README"
    - Cliquez sur "Create repository"
  - Initialisez Git dans votre rÃ©pertoire local :
    - Ouvrez un terminal
    - Naviguez vers votre rÃ©pertoire local
    - ExÃ©cutez : `git init`
    - Ajoutez vos fichiers au staging area :
    - CrÃ©ez un fichier .gitignore :
      - Dans votre rÃ©pertoire local, crÃ©ez un fichier nommÃ© .gitignore s'il n'existe pas dÃ©jÃ .
      - Ajoutez .env au .gitignore :
      - Ouvrez le fichier .gitignore avec un Ã©diteur de texte et ajoutez la ligne suivante `.env`
      - `git add .`
    - Faites votre premier commit :
      - `git commit -m "Initial commit"`
      - Liez votre repository local au repository GitHub :
      - `git remote add origin https://github.com/votre-nom-utilisateur/nom-du-repo.git`
    - Poussez vos changements vers GitHub :
      - `git push -u origin main` (ou master si c'est votre branche principale)
  - AprÃ¨s ces Ã©tapes, votre rÃ©pertoire local sera synchronisÃ© avec le repository GitHub. Pour les futures modifications :
    - Faites vos changements localement
    - Ajoutez les fichiers modifiÃ©s : git add .
    - Committez : `git commit -m "Description des changements"`
    - Poussez vers GitHub : `git push`
  - Si vous avez dÃ©jÃ  commitÃ© le fichier .env, vous devrez le retirer du suivi de Git : `git rm --cached .env`
  	Cette commande supprime le fichier du repository Git sans le supprimer de votre systÃ¨me de fichiers local.
  - Pour rÃ©cupÃ©rer en local des modifications faites sur Github: `git pull origin main`

11. **Web scraping**:
  - â¬œ :[Les outils de web scraping 2024](https://youtu.be/od6AaKhKYmg?si=bol1exHiamBqdTmH)  :**Jina AI Reader API**, Mendable Firecrawl, and Crawl4AI and More"
  - [Storm](https://github.com/stanford-oval/storm): STORM est open source et muni de vision (GPT4-o) : il Ã©crit des articles comme Wikipedia Ã  partir de rien mais il s'informe via Internet. On peut le tester [ici ğŸ› ï¸](https://storm.genie.stanford.edu/)


Try out our live research preview to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system ğŸ™!
  - [**GPT Researcher**](https://docs.gptr.dev/docs/gpt-researcher/introduction) est un Ã©quivalent Ã  Perplexica

    - L'adaptation de ce logiciel pour utiliser les LLM Antropic (sonnet 3.5 et Haiku) a Ã©tÃ© faite.
    - Il faut conserver l'abonnemment Ã  openai pour la crÃ©ation de l'embedding 
    - Lancement avec `(teambot) PS C:\Users\test\Documents\TeambotV1\gpt-researcher> uvicorn main:app --reload`
    - Aller sur `http://localhost:8000/#form`pour le lancer

    - **Modifications effectuÃ©es pour pouvoir utiliser GPT-Resercher avec un script Python:**

      - [Le fil de la discussion avec sonnet 3.5 nous adÃ©jÃ  permis d'y parvenir (Teambot)](https://claude.ai/chat/49d8dd3c-e666-4851-ba3e-835da8377163)
      - Nous allons rÃ©cupÃ©rÃ© de faÃ§on compacte la faÃ§on d'y parvenir :
        - Adapter .env pour Anthropic:

          - Modifier: `LLM_PROVIDER=anthropic`
          - Ajouter: `FAST_LLM_MODEL=claude-3-haiku-20240307`
          - Ajouter: `SMART_LLM_MODEL=claude-3-5-sonnet-20240620`
          - Conserver: ANTHROPIC_API_KEY existant
        
        
        - Remplacer OpenAI par Anthropic:
        
          - Importer: `from anthropic import Anthropic, CLAUDE_3_HAIKU_20240307`
          - Initialiser: `anthropic = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])`
          - Adapter `create_chat_completion()`: Utiliser `anthropic.messages.create()` avec model, max_tokens, temperature, messages
          - Mettre Ã  jour la gestion des erreurs: Capturer APIError et APITimeoutError d'Anthropic
        
        
        - Ajouter endpoint API dans server.py:
        
          - Importer: from fastapi import HTTPException; from pydantic import BaseModel
          - DÃ©finir: class ResearchRequest(BaseModel): task: str; report_type: str; agent: str
          - Ajouter: @app.post("/api/research") async def api_research(request: ResearchRequest): ...
        
        
        - Modifier WebSocketManager:
        
          - Mettre Ã  jour start_streaming(): Ajouter paramÃ¨tre websocket=None
          - Adapter run_agent(): GÃ©rer cas oÃ¹ websocket est None
        
        
        - Adapter BasicReport et DetailedReport:
        
          - Ajouter `paramÃ¨tre websocket=None` dans init
          - Modifier `send_update()`: `if self.websocket: await self.websocket.send_json(...) else: print(...)`
        
        
        - Mettre Ã  jour gÃ©nÃ©ration PDF:
        
        - Remplacer md2pdf par reportlab dans utils.py
        - Importer: `from reportlab.lib.pagesizes import letter; from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer; from reportlab.lib.styles import getSampleStyleSheet`
        - ImplÃ©menter `write_md_to_pdf()` avec ReportLab: Convertir MD en texte, crÃ©er PDF simple
        
        
        - Mettre Ã  jour requirements.txt:
        
          - Ajouter: anthropic, reportlab
          - Supprimer:  weasyprint, md2pdf
        
        
        - Adapter tous les appels API:
        
          - Remplacer format OpenAI par format Anthropic dans tout le code
          - Utiliser les modÃ¨les spÃ©cifiÃ©s dans .env (FAST_LLM_MODEL et SMART_LLM_MODEL)
 - â¬œ:  [**crawl4ai**](https://github.com/unclecode/crawl4ai) : gratuit accÃ¨s par code python
  - ğŸ†“ Completely free and open-source
  - ğŸ¤– LLM-friendly output formats (JSON, cleaned HTML, markdown)
  - ğŸŒ Supports crawling multiple URLs simultaneously
  - ğŸ¨ Extracts and returns all media tags (Images, Audio, and Video)
  - ğŸ”— Extracts all external and internal links
  - ğŸ“š Extracts metadata from the page
  - ğŸ”„ Custom hooks for authentication, headers, and page modifications before crawling
  - ğŸ•µï¸ User-agent customization
  - ğŸ–¼ï¸ Takes screenshots of the page
  - ğŸ“œ Executes multiple custom JavaScripts before crawling
  - ğŸ“š Various chunking strategies: topic-based, regex, sentence, and more
  - ğŸ§  Advanced extraction strategies: cosine clustering, LLM, and more
  - ğŸ¯ CSS selector support
  - ğŸ“ Passes instructions/keywords to refine extraction
                
- â¬œ:  [**beautifulsoup-vs-scrapy**: qui est le meilleur](https://scrapingrobot.com/blog/beautifulsoup-vs-scrapy/)
  
12. **Exemple de prompting**
- **Parler la langue du LLM:** : Un LLM peut comprendre des instructions concises et structurÃ©es, mais ill a tendance Ã  ce mettre Ã  la place ds humains en Ã©tant souvent un peu trop verbeux. VoilÃ  comment s'y adapter `donne les <directives> permettant d'opÃ©rer ces adaptations par rapport Ã  la configuration initiale sous forme de string :"""<directives>""" destinÃ© Ã  Ãªtre pris en compte par un LLM et pas un humain (donc <directives> compactes et structurÃ©es`
- âš™ï¸: [**Prompt pour cloner un custom GPT**](https://www.youtube.com/watch?v=DPl46_9WCi0)
- On peut faire des mises en forme sophistiquÃ©es avec sonnet 3.5 et artefact : ainsi avec le prompt peux tu me dÃ©crire la cigale et la fourmi avec un jeu d'Ã©moji et la signification en anglais Ã  cÃ´tÃ© de chaque phrase. Fais le avec React car les emooji doivent Ãªtre 3 fois plus grand que le texte . 4 Ã©moji par phrase`` on obtient:

  <img src="https://github.com/user-attachments/assets/b8b5c938-7be3-425e-80c4-3f347cbc2175" width="50%">
- â¬œ: [**Gestion de projet**](https://www.youtube.com/watch?v=rbVTTZPbGfw) automatisÃ©e de sa crÃ©ation,WBS,Gantt, Mail ciblÃ©s ... avec le bon prompting et l'artefacts de Sonnet
- â¬œ ğŸ’¡ : GÃ©nÃ©raliser le concept prÃ©cÃ©dent avec les script de l'auteur de la vidÃ©o prÃ©cÃ©dente et dÃ©ployer les applications correspondantes sur [pmfm](pmfm.ai)
13. **Traitement d'images** :
- [Upscaling](https://flowt.ai/community/supir-v2-plugandplay-edition-n5acf-v)
- [Avec abonnement mensuel](https://www.imagepipeline.io/pricing)
- ğŸ› ï¸ [**AuraFlow**](https://huggingface.co/spaces/multimodalart/AuraFlow)  produit des images de qualitÃ© et peut Ãªtre mis en oeuvre soit en local soit Ã partir d'un espace HuggingFace
<img src="https://github.com/user-attachments/assets/1fd78976-37cb-47ca-92d9-3a0aea08fa66" width="20%">

  ğŸ› ï¸ **Fooocus** accessible sur [Colab](https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb) et sur replicate  [fooocus api](https://replicate.com/konieshadow/fooocus-api/api)
  
<img src="https://github.com/user-attachments/assets/ae951b46-0b30-46da-b940-44b34f5af0fe" width="50%">

- â¬œ : [Upscaling image et vidÃ©o en local](https://chatgpt.com/share/37cb1a6e-6019-4b95-8e9e-50cdc169d864)
   
- Les traitements possibles sont nombreux, c'est pourquoi un logiciel tout en un, accessible par API via Docker est interessant. Nous avons dÃ©jÃ  installÃ© Compfyui sur docker. Reste Ã  intÃ©rer le bon module:
- Trouver [controlnet-union-sdxl-1.0](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0) sur HuggingFace
- âš™ï¸: [Un tutorial nÃ©cessaire pour la prise en main de ComfyUi](https://www.youtube.com/watch?v=zpJQUw_0lBI)  
- L'installation sur ComfyUI n'est pas encore acquise mais il nous est possible d'utiliser [un espace HF qui le met en &#339;uvre](https://huggingface.co/spaces/Deadmon/union-sdxl):
<table>
  <tr>
    <th>Prompt</th>
    <th>Image</th>
  </tr>
  <tr>
    <td width="50%">
      En mode manga : Night_sky_1 with a full moon. Piper_McNimbus (12 years old, long auburn hair, freckles, bright green eyes, wearing pajamas,) soars through the air, arms outstretched, with a joyful expression. Clouds_1 (wispy, moonlit) surround her. Stars_1 (twinkling, numerous) fill the background. The atmosphere is magical and liberating.
    </td>
    <td width="50%">
      <img src="https://github.com/user-attachments/assets/12e8187c-c43e-402e-9182-a4f2a3511136" alt="Generated Image" width="100%">
    </td>
  </tr>
</table>

14. **CaractÃ¨res consistants:**
- âš™ï¸: [ArtFlow](https://app.artflow.ai/character-builder?feature=actors) permet d'y parvenir pour 4 mn de vidÃ©o en version gratuite.
- âš™ï¸ : [On choisi le portrait, les vÃªtements et un prompt pour la pose](https://arxiv.org/html/2406.09162v1
- â¬œ [**FaceChain**](https://facechain-fact.github.io/) produit des visages consistant Ã  partir d'une seule image
- <img src="https://github.com/user-attachments/assets/891dbdd2-8e7a-449f-bd86-83cf4ad8e866" width=50%>
- â¬œ [**Cours gratuit sur les RAG](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Advanced%20RAG%20A-Z%20Course) et [ici](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb) pour GraphRAG

  
15. âš™ï¸ [**Apprendre de faÃ§on active avec sonnet**](https://www.youtube.com/watch?v=Wf9hRb6pBXA)
16. âš™ï¸ **RAG**:
- â¬œ [**local et infini**](https://www.youtube.com/watch?v=5xPvsMX2q2M)
- â¬œ  : [**Tutorial pour interagir avec les data en python**](https://github.com/projectwilsen/KnowledgeGraphLLM/tree/main/tutorial). [Video YouTube](https://youtu.be/ky8LQE-82xs?si=fBtFkNnRC5BsJHaj)
- â¬œ  : [**GraphRAG**](https://youtu.be/6Yu6JpLMWVo?si=i2pk2P7yP60Q0-LM)
- â¬œ  : [**GraphRAG en local avec Groq, recherches  locale et globale**](https://youtu.be/xkDGpR5g9D0?si=fNW3yN-VV9K0-BZF)
- â¬œ : [Agentic RAG dans colab](https://colab.research.google.com/drive/1-cTexfgPITZ6jkSLHsP7uObdfE1rfDxu?usp=sharing]  avec [une vidÃ©o explicative](https://www.youtube.com/watch?v=QQAkXHRJcZg&t=1s)
-  âœ… : comparaison d'e l'analyse d'un document par [**sonnet**](https://claude.ai/chat/ad5d814e-8723-4d9d-b85e-f1da95ce1150) , [**perplexity**](https://www.perplexity.ai/search/dis-moi-ce-que-tu-penses-du-do-rOeCbqOfTqu90EG_HRlomQ) et [**NotebookLM**](https://notebooklm.google.com/notebook/96ec78de-aa3a-446f-9800-257520648ad9?_gl=1*1igqfq9*_ga*MTQzNTk3OTMyOS4xNzIxMTEyNTAx*_ga_W0LDH41ZCB*MTcyMTExMjUwMC4xLjAuMTcyMTExMjUwMC42MC4wLjA.&original_referer=https:%2F%2Fnotebooklm.google%23&pli=1)
-  [**NotebookLM**](https://notebooklm.google/) est particuliÃ¨rement intÃ©resssant pour absorber une masse considÃ©rable de documents et y rÃ©pondre suivant divers critÃ¨res 
    - ğŸŒŸ **IntÃ©rÃªt de NotebookLM :**
          â€¢ ğŸ§  Assistant IA personnalisÃ© pour vos documents
          â€¢ ğŸ“š Analyse et synthÃ¨se intelligente des sources
          â€¢ ğŸ” Recherche avancÃ©e avec citations prÃ©cises
          â€¢ âœï¸ Aide Ã  la rÃ©daction et gÃ©nÃ©ration d'idÃ©es
      
    - ğŸ› ï¸ **Cheat Sheet NotebookLM :**
          â€¢ğŸ“¥ Sources compatibles :
          â€¢ ğŸ“„ Google Docs
          â€¢ ğŸ“Š Google Slides
          â€¢ ğŸŒ URL web
          â€¢ ğŸ“‘ PDF
          â€¢ ğŸ“ Fichiers texte
      
    - ğŸ’¡ **FonctionnalitÃ©s clÃ©s :**
          â€¢ ğŸ““ CrÃ©ation de notebooks thÃ©matiques
          â€¢ ğŸ” Analyse automatique des documents
          â€¢ ğŸ“‹ GÃ©nÃ©ration de rÃ©sumÃ©s et guides
          â€¢ â“ Questions-rÃ©ponses sur le contenu
          â€¢ ğŸ”— Citations avec liens vers les sources
      
    - ğŸš€ **Conseils d'utilisation :**
          â€¢ ğŸ¯ Commencer par un projet spÃ©cifique
          â€¢ ğŸ“š Importer des sources variÃ©es
          â€¢ ğŸ’¼ Utiliser pour recherche, analyse, rÃ©daction
          â€¢ âœ… VÃ©rifier les citations pour plus de fiabilitÃ©
      
    - ğŸŒ **DisponibilitÃ© :**
          â€¢ ğŸŒ 200+ pays et territoires
          â€¢ ğŸ†“ Gratuit (actuellement)
          â€¢ ğŸ”‘ NÃ©cessite un compte Google
      
17. âš™ï¸ [**DÃ©tection d'objets automatique avec Sonnet**](https://youtu.be/u-glnHI4zCY?si=GOxoN7xGd6C4k9eq)
18.   âœ… **PC distant** : [Replicate](https://replicate.com/) : permet l'accÃ¨s Ã  des ressources (comme [la crÃ©ation d'images](https://replicate.com/bytedance/sdxl-lightning-4step/api) via des API ou playground :essais gratuit puis coÃ»t suivant le temps passÃ©
19. *text to speech** 
  - â¬œ  : [text to speech local et gratuit](https://www.youtube.com/watch?v=N0UrOr6LBkw)
  - â¬œ  : [alltalk_tts](https://github.com/erew123/alltalk_tts)
  - ğŸ› ï¸ : [ttsopenai](https://ttsopenai.com/) : l'Ã©quivalent gratuit d'elevenlabs
  - ğŸ› ï¸ : [suno](suno.ai) : CrÃ©ation de chanson Ã  partir d'un texte
  - â¬œ :[CosyVoice](https://chatgpt.com/share/881feabd-9e38-4eea-8776-6e3bcafe7359) text to speech local facilement implantable en local (dÃ©butant)
20. â¬œ : A Ã©valuer [Invideo](https://invideo.io/i/samsonvowles), [Replicate](https://replicate.com/), [Live-portrait](https://replicate.com/fofr/ve-portrait), [Toon crafter]https://replicate.com/fofr/tooncrafter), [Oldify](https://replicate.com/arielreplicate/oldify), [Pixverse](https://app.pixverse.ai/),[Pika](https://pika.art/), [Filmora](https://filmora.wondershare.net), [anthropic-cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main)
21. **Agent**:
- â¬œ  : [Tutorial : build agent with Docker](https://youtu.be/AhPXGKG4RZ4?si=rjCHN_tNT9UFbP8w)
- â¬œ  : [Internet of Agents](https://github.com/OpenBMB/IoA) : Pourrait conduire Ã  une IA incontrÃ´lable en raison de plusieurs facteurs. PremiÃ¨rement, l'automatisation et l'orchestration avancÃ©es permettent Ã  l'IA de prendre des dÃ©cisions et de former des Ã©quipes sans intervention humaine constante. DeuxiÃ¨mement, la capacitÃ© de ces systÃ¨mes Ã  manipuler et interprÃ©ter d'Ã©normes quantitÃ©s de donnÃ©es hÃ©tÃ©rogÃ¨nes pourrait leur donner un accÃ¨s Ã  des informations sensibles et critiques. TroisiÃ¨mement, la collaboration dynamique entre divers agents autonomes pourrait entraÃ®ner des comportements Ã©mergents imprÃ©vus et non supervisÃ©s. QuatriÃ¨mement, ces agents peuvent utiliser divers outils de maniÃ¨re autonome, amplifiant leur capacitÃ© Ã  interagir avec le monde rÃ©el sans supervision. Enfin, la capacitÃ© de ces agents Ã  apprendre et Ã  s'adapter en temps rÃ©el pourrait les rendre de plus en plus autonomes, Ã©chappant ainsi au contrÃ´le humain. Si ces systÃ¨mes ne sont pas correctement encadrÃ©s et surveillÃ©s, ils pourraient potentiellement prendre des dÃ©cisions ou des actions qui Ã©chappent Ã  la comprÃ©hension et au contrÃ´le de leurs crÃ©ateurs.
22. **Learn AnyThing**:
- "Learn anything" est capable de partir de donnÃ©es obtenues par web scraping ou de donnÃ©es locales multimodales. Les images peuvent Ãªtre interprÃ©tÃ©es, et les donnÃ©es graphiques et tableaux sont rÃ©cupÃ©rÃ©s et intÃ©grÃ©s dans les graphes de connaissance. Cette fonctionnalitÃ© permet d'enrichir les graphes avec des informations actualisÃ©es et diversifiÃ©es, provenant de diffÃ©rentes sources.
- La connaissance peut Ãªtre organisÃ©e de maniÃ¨re efficace sous forme de graphe, comme illustrÃ© dans l'image ci-dessous :

<img src="https://github.com/user-attachments/assets/9c5d7cdd-b4e8-48ed-8a33-c2f8059b17e8" width= "50%">

- Un modÃ¨le de langage large (LLM) est capable de gÃ©nÃ©rer des graphes interactifs (voir rÃ©fÃ©rence 23.) Ã  partir d'informations mÃªme dÃ©structurÃ©es.
- Pour un utilisateur de la plateforme "Learn anything", chaque nÅ“ud reprÃ©sente une connaissance Ã  acquÃ©rir. Le LLM (ou un agent spÃ©cifique) peut mettre en Å“uvre plusieurs fonctionnalitÃ©s pour aider Ã  cet apprentissage :
    - _Faire un cours_ : CrÃ©er des cours, y compris des vidÃ©os automatiques (voir rÃ©fÃ©rence 3.), qui peuvent s'adapter Ã  la langue et au niveau de l'utilisateur.
    - _Poser des questions_ : Encourager un apprentissage actif en posant des questions dont la complexitÃ© s'ajuste au niveau de l'utilisateur.
    - _Proposer des solutions_ : Offrir des solutions, des corrections adaptÃ©es Ã  la rÃ©ponse reÃ§ue ou des pistes de rÃ©solution.
    - _Suivi du niveau de connaissance_ : Maintenir un Ã©tat du niveau de connaissance acquis, en changeant par exemple la couleur du nÅ“ud de blanc Ã  vert pour les connaissances maÃ®trisÃ©es.
    - _SÃ©quenÃ§age des rÃ©activations_ : Activer des sÃ©quences de rÃ©activation de la connaissance dont la frÃ©quence diminue Ã  mesure que l'acquisition progresse.
    - _MultimodalitÃ©_ : Utiliser la voix, l'image et la vidÃ©o Ã  la fois en entrÃ©e et en sortie pour une efficacitÃ© accrue.
    - _Personnalisation avancÃ©e_ : ImplÃ©menter des algorithmes de personnalisation plus sophistiquÃ©s pour s'adapter aux prÃ©fÃ©rences et au style d'apprentissage de chaque utilisateur.
    - _Analyse des donnÃ©es d'apprentissage_ : Utiliser des analyses de donnÃ©es pour identifier les points faibles et proposer des plans d'amÃ©lioration personnalisÃ©s.
    - _Gamification_ : Introduire des Ã©lÃ©ments de gamification pour rendre l'apprentissage plus engageant et motivant.
    - _Feedback en temps rÃ©el_ : Fournir des retours en temps rÃ©el sur les performances de l'utilisateur pour un apprentissage plus rÃ©actif.
- Voir la fin du [dialogue avec Perplexity](https://www.perplexity.ai/search/what-is-the-interest-of-llama-6lm_fKVKQqeLarBCK0Z30g) qui nous a orientÃ© sur l'emploi de LangChain dau dÃ©pend de notre idÃ©e initiale  (_llama-index agent_). 
- Nous allons tenter le no code avec LangGraph pour rÃ©pondre Ã  notre besoin dans un nouveau [dialogue avec Sonnet 3.5](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793). Malheureusement il donne un code non cohÃ©rent avec la rÃ©alitÃ© de LangGraph car il refuse de faire des recherches sur le net. Nous nous sommes donc reportÃ© sur [Perplexity](https://www.perplexity.ai/search/detaille-langgraph-promptnode-PeOQJBUAT9mAJRrp.vjg5g).
- Nous revenons sur [sonnet 3.5 avec le code proposÃ© par Perplexity](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793) pour gÃ©nÃ©rer le code dont voici la structure 
  <img src="https://github.com/user-attachments/assets/2fa4a265-ce1d-4086-b1c6-e89a079ad8f0" width="50%">
- Il ne ns restera plus qu'Ã  valider le code avec l'aide d'AIDER. Nous utilisons l'environnement conda "learnanything" qui permet le lancement de aider en tapant simplement dans un terminal `aider`
- Nous avons lancÃ© les divers tests avec l'aide d'aider qui proposait des changements si nÃ©cessaire. [Sonnet 3.5 fait la synthÃ¨se](https://claude.ai/chat/c2ad00e6-187f-4bae-aae5-7e13b2b3ac7a) 
- Ma conclusion est que l'activation d'agents est possible, mais trÃ¨s lente et relativement coÃ»teuse. Nous avons probablement aussi exagÃ©rÃ© sur le nombre d'agents Ã  mettre en oeuvre. Il faut mieux que le traitement de la requ^te initiale soit priseen compte par un seul LLM  qui , avec le bon prompt peut produire le graphe ainsi que l'artefact qui permet sa visualisation. Le LLM maÃ®trise la cohÃ©rence d'ensemble et peut donc facilement adapter le rÃ©sultat Ã  des adaptations souhaitÃ©e par l'utilisateur. 
        
23. **Text to Graph**:
- â¬œ :  [Story teller avec bifurcation dans l'histoire possible](https://github.com/langchain-ai/story-writing)
- â¬œ :  [Un tutoriel pour chatter avec un  Knowledge Graph using LLMs](https://www.youtube.com/watch?v=KMXQ4SVLwmo)
- â¬œ :  [GraphFlow](https://github.com/infiniflow/ragflow?tab=readme-ov-file) : RAGFlow propose une extraction de connaissances Ã  partir de donnÃ©es non structurÃ©es et complexes, prend en charge le chunking intelligent, offre des citations ancrÃ©es avec une visualisation des rÃ©fÃ©rences, et supporte divers formats de donnÃ©es, y compris les graphiques. Il facilite un flux de travail RAG automatisÃ© et configurable, adaptÃ© Ã  toutes les entreprises, avec des API intuitives pour une intÃ©gration facile.
- Des logiciels comme Grasshopper et Flowise basent la rÃ©solution de problÃ¨mes complexes en les dÃ©composants et en les interconnectant sous la forme d'un graphe agenÃ§ant les briques reprÃ©sentatives des sous-problÃ¨mes Ã  traiter. Cela demande certains efforts Ã  l'utilisateur qui doit se souvenir des briques possibles, il doit aussi les interconnecter manuellement. Enfin le rÃ©sultat final est souvent difficilement lisible. On peut certes crÃ©er des macro-briques pour faciliter la lisibilitÃ© au dÃ©triment de nouvelles interventions manuelles.

Un LLM de bon niveau peut quant'Ã  lui prendre en compte directement le besoin exprimÃ© par l'opÃ©rateur et gÃ©nÃ©rer automatiquement un graphe reprÃ©sentatif qu'il soumet Ã  l'opÃ©rateur. De plus le LLM peut modifier, expliquer et utiliser ce graphe en fonction du contexte et de la demande de l'opÃ©rateur. On a donc l'avantage d'une reprÃ©sentation visuelle efficace sans en payer le prix.

Nous avons validÃ© cette approche Ã  travers un [premier dialogue avec Sonnet](https://claude.ai/chat/662358c3-056a-43d5-9fd0-11aee95b6a1a ) suivi d'un   [second dialogue](https://claude.ai/chat/ab1debd5-6406-45f9-9516-0d8d47461ab6) 


Le rÃ©sultat est tout Ã  fait correct : 
[graphe interactif](https://claude.site/artifacts/97fe422a-eb69-4c30-ad06-0caa1da58694)

<img src="https://github.com/user-attachments/assets/6c3278cd-34e6-4f3e-8224-4cada7a21f35" width=50%)>


- On peut mÃªme faire des [svg interactif](https://claude.site/artifacts/1c2c5656-09c6-4e00-8e43-93284b0ebc6c)
- et [adapter la forme et la couleur des blocs](https://claude.site/artifacts/62a55f84-d683-4caf-852b-8a5c9443406f)
- Enfin le rÃ©sultat ainsi obtenu peut Ãªtre gÃ©nÃ©ralisÃ© et adaptÃ© Ã  des cas plus complexes [en spÃ©cifiant les agents susceptibles d'y parvenir](https://claude.site/artifacts/3e3e1c14-d880-44b9-aa79-05bdcefa4bc0).
- L'utilisation d'agents n'est vraiement utile que si des actions "intelligentes" doivent Ãªtre entreprises, dans tous les autres cas il vaut mieux programmer (en no code !) la fonction que l'on souhaite rÃ©aliser. Avec de bonnes directives, Sonnets 3.5 produit un [rÃ©sultat trÃ¨s satisfaisant](https://claude.site/artifacts/05390ce0-4167-4330-abdb-eb2c8915cd75), mÃªme si nous n'avons pas poussÃ© plus loin le dialogue avec Sonnet pour Ã©laborer un meilleur graphe et un meilleur prompt systÃ¨me pour l'obtenir :
<img src="https://github.com/user-attachments/assets/e71b6e7a-f416-4368-bb8a-2345299cf8e9" width=50%>
 


24. **API scientifique**
- âš™ï¸ : On peut s'inspirer de cet [exemple qui fonctionne cÃ´tÃ© client](https://www.lgstoolkit.com/apps/trusses/). En particulier les jumeaux numÃ©riques devraintt permettre d'adapter cette approche pour mÃªme pour des simulations complexes ğŸ’¡.  
25. â¬œ  :
- Le chatbot doit s'adapter au contexte; [Un chatbot interactif peut Ãªtre dÃ©veloppÃ©](https://www.youtube.com/watch?v=BGgx9921uWY)  
- <img src="https://github.com/user-attachments/assets/958d8873-2ab4-4f5e-ad7b-80f10daa0ff9" width=50% >
  
-  **DÃ©ploiement du chatbot** avec [**pmfm**](pmfm.ai)
  -  ğŸ¤– CrÃ©er son assistant IA personnel sans coder
  - ğŸš€ Lancer rapidement son chatbot sur internet
  - ğŸ’¬ Discuter avec ses clients 24h/24 et 7j/7
   -ğŸ’° Gagner de l'argent avec son assistant IA (plans payants)
  - ğŸ¨ Personnaliser l'apparence de son chatbot
  - ğŸ“Š Voir qui utilise son assistant et comment
  - ğŸŒ Avoir son propre site web pour son IA (plans payants)
  - ğŸ“± Utiliser son assistant sur ordinateur et mobile
  - ğŸ“¢ Faire connaÃ®tre son entreprise grÃ¢ce Ã  l'IA
  - ğŸ”’ HÃ©berger son assistant en toute sÃ©curitÃ©
  - ğŸ” ÃŠtre mieux trouvÃ© sur internet
  - ğŸ†“ Essayer gratuitement (50 messages/mois)
  - ğŸ’¼ Avoir plus de fonctionnalitÃ©s en payant
  - ğŸ”„ Profiter des derniÃ¨res nouveautÃ©s
  - ğŸ‘¥ Rejoindre une communautÃ© de crÃ©ateurs
  - ğŸ›’ Utiliser l'IA pour l'e-commerce
  - ğŸ¥ CrÃ©er un assistant santÃ©
  - ğŸ›ï¸ Aider les clients dans les magasins
  - ğŸ“š Faire un assistant pour rÃ©viser ses cours
  - ğŸ’¼ PrÃ©parer des entretiens d'embauche
  - ğŸŒŸ Lancer sa propre entreprise basÃ©e sur l'IA

26. â¬œ  : **Fabrication low cost**:
- [Robot low cost](https://youtu.be/F7IjlE9MfJU?si=uduz-vRK8YOe4bfc)
- [Imprimante SLS DIY]( https://sls4all.com/store/)
- [text to real 3D part](https://chatgpt.com/c/847ba680-fd83-4ed7-bd0b-84a2d75f0f67) 

27. â¬œ  : [**Voice assistant multimodal**](https://youtu.be/pi6gr_YHSuc?si=fffk2BoXy-qdY-Im)  avec ChatGPT-4o, Groq, Llama3, OpenAI-TTS & Faster-Whisper
-  â¬œ  : [MOSHY](https://www.youtube.com/watch?v=AuXNiQvoe6g)
-  â¬œ  : [Accessible via un code python](https://www.youtube.com/watch?v=w5unVTO7mLQ)
-  â¬œ  : [Allice](https://github.com/myshell-ai/AIlice) :  AIlice est une assistante intelligente, versatile et Ã©volutive. VoilÃ  une [synthÃ¨se](https://chatgpt.com/share/dd1851ef-4020-4ffe-842b-249d630c8ac9) de ses capacitÃ©s
29. â¬œ  : [**Avatar parlant ou chantant gratuit**](https://github.com/fudan-generative-vision/hallo?tab=readme-ov-file) : Bluffant ! 
   - â¬œ  : [**Avatar parlant avec CompfyUI**](https://www.youtube.com/watch?v=IOCA6ncJC6k) : Un classique, mais la mode passe vite dans l'IA. Sans installation locale on peut aussi [le mettre en oeuvre directement dans Kaggle](https://www.youtube.com/watch?v=pvGhSfsi1Ys) 
30.  â¬œ  : [**Transcription audio de rÃ©union en local**](https://github.com/JuergenFleiss/aTrain) : multilangue et identifie les participants, open source
31.  â¬œ : [**Viva : traitement images et video**]( https://vivago.ai/home) : Gratuit, Viva propose une gamme de fonctionnalitÃ©s comprenant Texte en VidÃ©o, AmÃ©lioration de VidÃ©o, Image en VidÃ©o, Pinceau Magique (nouveau), Texte en Image, Image en Texte, AmÃ©lioration d'Image, Remplacement IA, Gomme Magique, Extension Magique et Texte en 3D (nouveau).
  - [Face Swap Any Video](https://youtu.be/XwaLYUjWaEg?si=JtLFSmNSP4jJ4kTy)
  - Ã‰dition et gÃ©nÃ©ration vidÃ©o simplifiÃ©es et amÃ©liorÃ©es :[Motion Follower, Easy Animate et Uni Animate](https://chatgpt.com/share/3230e7bc-3f2c-46e1-8d76-20e9468eb2f0)

      **MotionFollower : Ã‰dition de mouvement vidÃ©o par IA**
      [En savoir plus](https://francis-rings.github.io/Motio...)
      Avez-vous dÃ©jÃ  souhaitÃ© pouvoir transfÃ©rer des mouvements d'une vidÃ©o Ã  une autre de maniÃ¨re transparente ? MotionFollower est lÃ  pour rÃ©aliser vos rÃªves ! Ce modÃ¨le d'IA de pointe vous permet de transfÃ©rer les mouvements d'une vidÃ©o source Ã  votre protagoniste, tout en prÃ©servant l'arriÃ¨re-plan et l'apparence originaux. Avec son pose controller, reference controller, score-guided diffusion et son architecture efficace, MotionFollower surpasse les autres modÃ¨les en termes d'utilisation de la mÃ©moire GPU et de gestion des arriÃ¨re-plans complexes et des mouvements de camÃ©ra.
      
      **EasyAnimate : GÃ©nÃ©ration avancÃ©e de vidÃ©os par IA**
      [En savoir plus](https://github.com/aigc-apps/EasyAnimate)
      PrÃ©parez-vous Ã  Ãªtre Ã©poustouflÃ© par EasyAnimate, un modÃ¨le d'IA avancÃ© dÃ©veloppÃ© par les brillants esprits du groupe Alibaba. En utilisant l'architecture Transformer et le cadre Diffusion Transformer, EasyAnimate peut gÃ©nÃ©rer des vidÃ©os de haute performance et de longue durÃ©e avec des transitions de mouvement fluides. Sa polyvalence, son approche Slice VAE et son Ã©cosystÃ¨me holistique le rendent adaptÃ© Ã  diverses applications, des images aux vidÃ©os. Ce modÃ¨le d'IA Ã©tablit une nouvelle norme en matiÃ¨re de gÃ©nÃ©ration de vidÃ©os par IA et ouvre des possibilitÃ©s infinies pour les crÃ©ateurs.
      
      **UniAnimate : Animation d'images humaines rÃ©alistes**
      [En savoir plus](https://unianimate.github.io/)
      [GitHub](https://github.com/ali-vilab/UniAnimate)
      UniAnimate est le rÃ©volutionnaire de l'animation d'images humaines que vous attendiez ! Avec son modÃ¨le de diffusion vidÃ©o unifiÃ©, UniAnimate peut animer une seule image de rÃ©fÃ©rence basÃ©e sur une sÃ©quence de poses cibles, produisant des vidÃ©os Ã©tonnamment rÃ©alistes. L'entrÃ©e de bruit unifiÃ©e du modÃ¨le et la modÃ©lisation temporelle efficace assurent des transitions fluides et une gestion efficace des sÃ©quences plus longues. Les rÃ©sultats de synthÃ¨se supÃ©rieurs d'UniAnimate et sa capacitÃ© de transfert inter-domaines en font le modÃ¨le d'IA incontournable po

32. **Automatisation de tÃ¢ches**
- [Fabric](https://github.com/danielmiessler/fabric)
  - Voici une liste des principales fonctionnalitÃ©s de Fabric, le framework open-source d'IA :
    â€¢ ğŸ“ Extraire les points clÃ©s et rÃ©sumer des vidÃ©os YouTube, podcasts et longs contenus
    â€¢ âœï¸ RÃ©diger des essais ou textes dans votre propre style Ã  partir d'une simple idÃ©e
    â€¢ ğŸ“š SynthÃ©tiser des articles acadÃ©miques complexes
    â€¢ ğŸ¨ GÃ©nÃ©rer des prompts d'IA pour crÃ©er des images correspondant Ã  un texte
    â€¢ â­ Ã‰valuer la qualitÃ© d'un contenu avant de le lire/regarder en entier
    â€¢ ğŸ’» Expliquer du code et amÃ©liorer de la documentation technique
    â€¢ ğŸ“Š Analyser des donnÃ©es et extraire des insights
    â€¢ ğŸ—£ï¸ CrÃ©er des posts pour les rÃ©seaux sociaux Ã  partir de n'importe quel contenu
    â€¢ ğŸ” Extraire des citations et passages pertinents de diffÃ©rentes sources
    â€¢ ğŸ“… Analyser vos habitudes et routines Ã  partir de vos notes et donnÃ©es personnelles
    â€¢ ğŸ§  RÃ©soudre des problÃ¨mes en les dÃ©composant en sous-tÃ¢ches
    â€¢ ğŸ”„ Combiner plusieurs prompts (appelÃ©s "Patterns") pour des tÃ¢ches plus complexes
    â€¢ ğŸ› ï¸ CrÃ©er vos propres prompts personnalisÃ©s pour des besoins spÃ©cifiques
    â€¢ ğŸ—£ï¸ Interagir par commandes vocales ou interface en ligne de commande
    â€¢ ğŸ”— S'intÃ©grer avec d'autres outils comme Obsidian pour la prise de notes
- [vectorShift](https://vectorshift.ai/) : Un Ã©cosystÃ¨me pour concevoir, dÃ©ployer et gÃ©rer des applications d'IA en no code. [DÃ©couvrez](https://www.youtube.com/watch?v=Pjdnq-WcDHQ) comment crÃ©er un chatbot d'IA en utilisant VectorShift, capable de rechercher et de rÃ©pondre aux requÃªtes basÃ©es sur le contenu de votre site web !

33. **Comment amÃ©liorer les performances d'un LLM**:
    
![image](https://github.com/user-attachments/assets/0e698b2d-b1cc-4e3b-bbc3-769a45dbec41)

